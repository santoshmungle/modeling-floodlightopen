{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "floodlight_data = pd.read_csv('../data/part2/complete_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 287521 entries, 0 to 287520\n",
      "Data columns (total 20 columns):\n",
      "floodlightOpenId                 287521 non-null object\n",
      "participantCreatedOn             287521 non-null object\n",
      "participantIsControl             287521 non-null bool\n",
      "participantCountryOfResidence    287521 non-null object\n",
      "participantSex                   287521 non-null object\n",
      "participantBirthYear             287521 non-null int64\n",
      "participantWeightLbs             287521 non-null float64\n",
      "participantHeightCms             287521 non-null float64\n",
      "testName                         287521 non-null object\n",
      "testCode                         287521 non-null object\n",
      "testMetricName                   287521 non-null object\n",
      "testMetricCode                   287521 non-null object\n",
      "testStartedAt                    287521 non-null object\n",
      "testEndedAt                      287521 non-null object\n",
      "testResultMetricId               287521 non-null int64\n",
      "testResultMetricCreatedOn        287521 non-null object\n",
      "testResultMetricTimestamp1       0 non-null float64\n",
      "testResultMetricTimestamp2       0 non-null float64\n",
      "testResultMetricValue            287521 non-null float64\n",
      "Unnamed: 19                      0 non-null float64\n",
      "dtypes: bool(1), float64(6), int64(2), object(11)\n",
      "memory usage: 42.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# info on data such as data type \n",
    "floodlight_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantBirthYear</th>\n",
       "      <th>participantWeightLbs</th>\n",
       "      <th>participantHeightCms</th>\n",
       "      <th>testResultMetricId</th>\n",
       "      <th>testResultMetricTimestamp1</th>\n",
       "      <th>testResultMetricTimestamp2</th>\n",
       "      <th>testResultMetricValue</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>287521.000000</td>\n",
       "      <td>287521.000000</td>\n",
       "      <td>287521.000000</td>\n",
       "      <td>287521.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287521.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1971.339325</td>\n",
       "      <td>171.369493</td>\n",
       "      <td>168.115317</td>\n",
       "      <td>163581.601466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.601507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.225201</td>\n",
       "      <td>54.048453</td>\n",
       "      <td>21.287611</td>\n",
       "      <td>94241.026701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243.872635</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1900.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1963.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>72587.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1971.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>171387.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1979.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>244971.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>317566.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45418.500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       participantBirthYear  participantWeightLbs  participantHeightCms  \\\n",
       "count         287521.000000         287521.000000         287521.000000   \n",
       "mean            1971.339325            171.369493            168.115317   \n",
       "std               12.225201             54.048453             21.287611   \n",
       "min             1900.000000              3.000000              1.000000   \n",
       "25%             1963.000000            135.000000            163.000000   \n",
       "50%             1971.000000            161.000000            170.000000   \n",
       "75%             1979.000000            190.000000            178.000000   \n",
       "max             2020.000000            530.000000            250.000000   \n",
       "\n",
       "       testResultMetricId  testResultMetricTimestamp1  \\\n",
       "count       287521.000000                         0.0   \n",
       "mean        163581.601466                         NaN   \n",
       "std          94241.026701                         NaN   \n",
       "min             66.000000                         NaN   \n",
       "25%          72587.000000                         NaN   \n",
       "50%         171387.000000                         NaN   \n",
       "75%         244971.000000                         NaN   \n",
       "max         317566.000000                         NaN   \n",
       "\n",
       "       testResultMetricTimestamp2  testResultMetricValue  Unnamed: 19  \n",
       "count                         0.0          287521.000000          0.0  \n",
       "mean                          NaN              28.601507          NaN  \n",
       "std                           NaN             243.872635          NaN  \n",
       "min                           NaN               0.000000          NaN  \n",
       "25%                           NaN               0.140000          NaN  \n",
       "50%                           NaN               1.000000          NaN  \n",
       "75%                           NaN               6.210000          NaN  \n",
       "max                           NaN           45418.500000          NaN  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic on the data\n",
    "floodlight_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data asessment:\n",
    "\n",
    "1. 3 columns are empty. So need to drop it.\n",
    "2. Many values are missing. \n",
    "3. Data types are not appropriate for multiple columns. Need to change it to datetime datatype and int data type\n",
    "4. Many variable are catgorical like Sex, ParticipantControl, Country. So need to do encoding like label encoding, and one hot encoding\n",
    "5. Some columns contains same info such as testname and testcode. Need to delete columns that have repetitive info.\n",
    "6. Need to reduce features by combining and cross-featuring. For example, getting BMI from weight and height, Age from birth years and testStartedAt etc.\n",
    "\n",
    "**Overall, we need to perform lots of feature engineering on the raw floodlight data to build classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this function is about data processing for building model: \n",
    "    1) dropping empty columns\n",
    "    2) changing data types for multiple columns\n",
    "    3) dropping columns that contain same info and unnecessary for building model\n",
    "    \n",
    "    Arguments:\n",
    "    floodlight_data -- pandas df of floodlight data\n",
    "    \n",
    "    Return:\n",
    "    processed_floodlight_data -- this is processed floodlight df  \n",
    "'''\n",
    "\n",
    "def data_processing(floodlight_data):\n",
    "    \n",
    "    # list of columns to drop\n",
    "    drop_col_list = ['testResultMetricTimestamp1','testResultMetricTimestamp2','Unnamed: 19', 'testCode', 'testMetricCode']\n",
    "    processed_floodlight_data = floodlight_data.drop(drop_col_list,axis=1,errors='ignore')\n",
    "    \n",
    "    # Apply datetime datatype to datetime columns\n",
    "    date_col_list = ['participantCreatedOn', 'testStartedAt', 'testEndedAt', 'testResultMetricCreatedOn']\n",
    "    processed_floodlight_data[date_col_list] = processed_floodlight_data[date_col_list].apply(pd.to_datetime)\n",
    "    \n",
    "    # Change to int data type \n",
    "    int_col_list = ['participantBirthYear', 'participantWeightLbs', 'participantHeightCms', 'testResultMetricId']\n",
    "    processed_floodlight_data[int_col_list] = processed_floodlight_data[int_col_list].apply(pd.to_numeric, downcast='integer')\n",
    "    \n",
    "    \n",
    "    return processed_floodlight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Feature engineering function steps:\n",
    "    1. Creating new feature from combining 'testName' and 'testMetricName'.\n",
    "    2. Creating new feature Body Mass Index (BMI) from height and weight. \n",
    "       Removing height and weight, keeping only BMI.\n",
    "    3. Creating new feature Age from birth year and testStartedAt. \n",
    "       Removing birth year and testStartedAt, keeping age only.\n",
    "    4. Creating new feature 'participantTestTime' from 'testEndedAt' and 'testStartedAt'\n",
    "    5. Implement one hot encoding for 'participantCountryOfResidence'. \n",
    "       Because country is categorical variable. \n",
    "    6. Categorical column grouping - because appearantly participant has participated in multiple test\n",
    "    7. Change Boolean True/False values to 1/0 for 'participantSex' \n",
    "       and 'participantIsControl'\n",
    "    8. Filling missing values with mean for 'testResultMetricValue'\n",
    "    9. Standardizing/normalizing the testResult Metric value\n",
    "    \n",
    "    Arguments:\n",
    "    processed_floodlight_data -- this is output of 'data_processing(floodlight_data)'\n",
    "    \n",
    "    Return:\n",
    "    floodlight_data_with_features -- dataframe with feature engineering \n",
    "    \n",
    "'''\n",
    "\n",
    "def feature_engineering(processed_floodlight_data):\n",
    "    \n",
    "    \n",
    "    # Combining testName and testMetricName into one feature 'test_long_name'\n",
    "    processed_floodlight_data['test_long_name'] = processed_floodlight_data['testName']+\"_\"+processed_floodlight_data['testMetricName']\n",
    "    floodlight_data_with_features = processed_floodlight_data.drop(['testName','testMetricName'], axis=1, errors='ignore')\n",
    "    \n",
    "    \n",
    "    ''' creating new feature BMI: \n",
    "    1. converting participant height and weight into kg and M\n",
    "    2. calculating BMI \n",
    "    3. deleting height and weight columns and keeping only BMI\n",
    "    '''\n",
    "    floodlight_data_with_features['participantHeightM'] = floodlight_data_with_features['participantHeightCms'].apply(lambda x: x/100)\n",
    "    floodlight_data_with_features['participantWeightKg'] = floodlight_data_with_features['participantWeightLbs'].apply(lambda x: x*0.454)\n",
    "    floodlight_data_with_features['participantBMI'] = floodlight_data_with_features['participantWeightKg']/(floodlight_data_with_features['participantHeightM'] **2)\n",
    "    floodlight_data_with_features.drop(['participantHeightM', 'participantWeightKg', 'participantHeightCms', 'participantWeightLbs'], axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate new feature Age from birth year and testStartedAt\n",
    "    floodlight_data_with_features['participantAge'] = [floodlight_data_with_features.testStartedAt[i].year - floodlight_data_with_features.participantBirthYear[i] \n",
    "                    for i in range(0, len(floodlight_data_with_features))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # total test time computed from 'testEndedAt' and 'testStartedAt'\n",
    "    floodlight_data_with_features['participantTestTime'] = (floodlight_data_with_features['testEndedAt'] - floodlight_data_with_features['testStartedAt']).apply(lambda x: x.seconds)\n",
    "    floodlight_data_with_features.drop(['testStartedAt', 'testEndedAt', 'participantBirthYear'], axis = 1, inplace=True, errors='ignore')\n",
    "    \n",
    "    \n",
    "    '''One hot encoding for 'participantCountryOfResidence'\n",
    "    'participantCountryOfResidence' is categorical variable. \n",
    "     We have categories with countries abbreviation such as IT, US, CA, PL etc\n",
    "     We don't need ordinal ranking. So, lets do One hot encoding instead of label encoding.\n",
    "    '''\n",
    "    # creating one hot encoding object\n",
    "    onehotencoder = OneHotEncoder()\n",
    "    \n",
    "    # reshaping 1D country array to 2D and as fit_transform expect 2D input and the fitting the input into object\n",
    "    participantCountry = onehotencoder.fit_transform(floodlight_data_with_features.participantCountryOfResidence.\\\n",
    "                                                     values.reshape(-1,1)).toarray()\n",
    "    \n",
    "    # creating one hot encoded dataframe\n",
    "    OneHot_df = pd.DataFrame(participantCountry, columns = [\"participantCountry_\"+str(int(i))\\\n",
    "                                                           for i in range(floodlight_data_with_features.shape[1])]) \n",
    "    \n",
    "    # concatenation of original df and one hot encoded df\n",
    "    floodlight_data_with_features = pd.concat([floodlight_data_with_features, OneHot_df], axis=1)\n",
    "    \n",
    "    # dropping column participantCountryOfResidence\n",
    "    floodlight_data_with_features.drop(['participantCountryOfResidence'], axis = 1, inplace=True, errors='ignore')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # getting list of categorical columns to group and then categorial grouping\n",
    "    columns_to_group = [col for col in floodlight_data_with_features.columns if col.startswith('participant')]\n",
    "    floodlight_data_with_features = floodlight_data_with_features.pivot_table(index=['floodlightOpenId']+columns_to_group, columns = ['test_long_name'], values= [\"testResultMetricValue\"])\n",
    "    floodlight_data_with_features = floodlight_data_with_features.reset_index()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Change True/False to 1/0\n",
    "    floodlight_data_with_features['participantSex'] = floodlight_data_with_features['participantSex'].replace({'female': 1, 'male': 0})\n",
    "    floodlight_data_with_features['participantIsControl'] = floodlight_data_with_features['participantIsControl'].astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # filling NA values of testResultMetricValue with its mean\n",
    "    floodlight_data_with_features['testResultMetricValue']= floodlight_data_with_features['testResultMetricValue'].\\\n",
    "    fillna((floodlight_data['testResultMetricValue'].mean()))\n",
    "    \n",
    "    \n",
    "    # standardizing/normalizing the testResult Metric value\n",
    "    floodlight_data_with_features[\"testResultMetricValue\"] = preprocessing.scale(floodlight_data_with_features[\"testResultMetricValue\"], \\\n",
    "                                                                             with_mean = True, with_std = True)\n",
    "    \n",
    "    return floodlight_data_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_testing_set(floodlight_data):\n",
    "    '''\n",
    "    Steps for creating training and test dataset for classifier\n",
    "    1) It calls two functions:\n",
    "       i) data_processing\n",
    "       ii) feature_engineering   \n",
    "    2) Target label encoding \n",
    "    3) Building train and test set using 'train_test_split function'\n",
    "    \n",
    "    Arguments: \n",
    "    floodlight_data -- pandas df of floodlight data\n",
    "    \n",
    "    Return:\n",
    "    X_train -- Training input data set, \n",
    "    X_test -- Test input data set\n",
    "    Y_train -- Training target data \n",
    "    Y_test -- Test target data\n",
    "    \n",
    "    ''' \n",
    "    # data processing: calling above data_processing function\n",
    "    processed_floodlight_data = data_processing(floodlight_data)\n",
    "    \n",
    "    # feature engineering: calls above feature_engineering function\n",
    "    floodlight_data_with_features = feature_engineering(processed_floodlight_data)\n",
    "    \n",
    "    # creating input data set from feature engineering data\n",
    "    X = floodlight_data_with_features.iloc[:, 3::]\n",
    "    \n",
    "    # target label data\n",
    "    Y = floodlight_data_with_features['participantIsControl']\n",
    "    \n",
    "    # Encoding target labels of \"participantIsControl\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    Y = le.fit_transform(Y)  \n",
    "    \n",
    "    # Build training and test set \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    print(\"train X shape: \" + str(X_train.shape))\n",
    "    print(\"test X shape: \" + str(X_test.shape))\n",
    "    print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "    print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape: (17587, 37)\n",
      "test X shape: (4397, 37)\n",
      "Y_train shape: (17587,)\n",
      "Y_test shape: (4397,)\n",
      "Logistic Regression classifier accuracy on training set: 74.86%\n",
      "Logistic Regression classifier accuracy on test set: 75.37%\n"
     ]
    }
   ],
   "source": [
    "'''Logistics Regression model\n",
    "'''\n",
    "# getting train and test dataset\n",
    "X_train, X_test, Y_train, Y_test = training_testing_set(floodlight_data)\n",
    "    \n",
    "# Logistic regression model\n",
    "model = LogisticRegression(solver=\"liblinear\", C=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "prediction = model.predict(X_test)\n",
    " \n",
    "# printing accuracy of train and test dataset    \n",
    "print('Logistic Regression classifier accuracy on training set: {:.4}%'.format(model.score(X_train, Y_train)*100))\n",
    "print('Logistic Regression classifier accuracy on test set: {:.4}%'.format(model.score(X_test, Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier based on Logistics Regression is giving almost 76% accuracy on test data set. We try another ML model like Random Forest which gives better results when many features are involved in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape: (17587, 37)\n",
      "test X shape: (4397, 37)\n",
      "Y_train shape: (17587,)\n",
      "Y_test shape: (4397,)\n",
      "Random Forest classifier training accuracy: 75.39%\n",
      "Random Forest classifier testing accuracy: 75.16%\n"
     ]
    }
   ],
   "source": [
    "'''Random Forest'''\n",
    "\n",
    "# getting train and test dataset\n",
    "X_train, X_test, Y_train, Y_test = training_testing_set(floodlight_data)\n",
    "\n",
    "'''random forest\n",
    "   hyperparamers: 1) n_estimators; 2) max_depth\n",
    "'''\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "forest.fit(X_train, Y_train)\n",
    "forest_predict = forest.predict(X_test)\n",
    "\n",
    "# printing accuracy of train and test dataset\n",
    "print('Random Forest classifier training accuracy: {:.4}%'.format(forest.score(X_train, Y_train)*100))\n",
    "print('Random Forest classifier testing accuracy: {:.4}%'.format(forest.score(X_test, Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest is giving almost same result as Logistics Regression**\n",
    "\n",
    "\n",
    "Random forest has two hyperparameter *n_estimator* and *max_depth*. Lets try to tune these two hyperparameters and see if we can improve the accuracy of our random forest classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This will take some time to get best parameter for random forest. So, have some patience :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will give us tuned hyperparameters for best accuracy of random forest classifier\n",
    "\n",
    "# in param_grid, you can enter range of 'n_estimators' and 'max_depth'\n",
    "# n_estimators = no. of trees in the forest\n",
    "# max_depth = max. number of levels in each decision tree\n",
    "# max_features = number of features considered for best splitting of a node\n",
    "# criterion = The function to measure the quality of a split. “gini” is for Gini impurity and “entropy” is for information gain.\n",
    "# cv = number of folds to use for cross validation\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [10, 11, 12, 13, 14, 15, 20, 25, 30],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_forest = GridSearchCV(estimator=forest, param_grid=param_grid, cv= 5)\n",
    "grid_forest.fit(X_train, Y_train)\n",
    "\n",
    "grid_forest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these tuned hyperparameters for our Random Forest. *max_depth ( max number of levels in each decision tree)* was only mentioned up to 30 for hypertuning. So, we could try more *max_depth* and see if it gets better result than 30 with parameters we got above.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Tuned Random Forest classifier for training set: 99.99%\n",
      "Accuracy of Tuned Random Forest classifier for test set: 87.4%\n"
     ]
    }
   ],
   "source": [
    "'''Tuned random forest hyperparamers: \n",
    "   1) n_estimators = 200; \n",
    "   2) max_depth = 60\n",
    "   3) max_features='auto\n",
    "   4) criterion='entropy'\n",
    "'''\n",
    "# getting train and test dataset\n",
    "#X_train, X_test, Y_train, Y_test = training_testing_set(floodlight_data)\n",
    "\n",
    "# max_depth = 60 worked slighly better for test set than max_depth = 30\n",
    "tuned_forest = RandomForestClassifier(max_features='auto', n_estimators=200, max_depth=60, \\\n",
    "                                      criterion='entropy', random_state=42)\n",
    "tuned_forest.fit(X_train, Y_train)\n",
    "tuned_forest_predict = tuned_forest.predict(X_test)\n",
    "\n",
    "# printing accuracy of train and test dataset\n",
    "print('Accuracy of Tuned Random Forest classifier for training set: {:.4}%'.format(tuned_forest.score(X_train, Y_train)*100))\n",
    "print('Accuracy of Tuned Random Forest classifier for test set: {:.4}%'.format(tuned_forest.score(X_test, Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With our tuned hyperparameters, Random Forest is performing very well compared to Logistics Regression. We almost got 100% accuracy for training dataset. And 87.4% accuracy for testing dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest is the Winner!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
